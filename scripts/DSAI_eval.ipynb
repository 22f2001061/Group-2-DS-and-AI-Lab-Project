{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw9hkMCh8N8k"
      },
      "outputs": [],
      "source": [
        "# Put near top of your notebook\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- 1) Run ultralytics validation to get detection metrics ----------\n",
        "def run_yolov8_val(model_path, data_yaml, imgsz=640, conf=0.25, iou=0.5, save_json=True, device=None):\n",
        "    \"\"\"\n",
        "    Runs YOLOv8 .val() to get mAP, precision, recall etc.\n",
        "    Returns: results object and metrics dict\n",
        "    \"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    if device:\n",
        "        model.to(device)\n",
        "    print(f\"Running val() for {model_path} ...\")\n",
        "    res = model.val(data=data_yaml, imgsz=imgsz, conf=conf, iou=iou, verbose=False)\n",
        "    # ultralytics returns object with metrics in res.metrics or res.box.map and res.box.pr... depends on version.\n",
        "    metrics = {}\n",
        "    try:\n",
        "        # Try common keys\n",
        "        metrics['mAP_50'] = float(res.box.map[0])            # map@0.5\n",
        "        metrics['mAP_50_95'] = float(res.box.map[1])        # map@0.5:0.95\n",
        "        metrics['precision'] = float(res.box.pr[0])\n",
        "        metrics['recall'] = float(res.box.pr[1])\n",
        "    except Exception:\n",
        "        # Fallback to printed JSON if available\n",
        "        if hasattr(res, 'metrics'):\n",
        "            metrics = dict(res.metrics)\n",
        "    # Optionally save JSON (for later per-class AP parsing)\n",
        "    if save_json and hasattr(res, 'saved_results') and 'results.json' in res.saved_results:\n",
        "        try:\n",
        "            out_json = res.saved_results['results.json']\n",
        "            with open(f\"{os.path.basename(model_path)}_val_results.json\",\"w\") as f:\n",
        "                json.dump(out_json, f)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return res, metrics\n",
        "\n",
        "# ---------- 2) Evaluate runtime (inference speed) and distance MAE ----------\n",
        "def eval_runtime_and_distance(model_path, video_or_image_list, focal_length_px=1000,\n",
        "                              known_heights_m=None, default_height_m=1.5, device=None, conf=0.25):\n",
        "    \"\"\"\n",
        "    Evaluate average inference time per frame and compute distance MAE if ground truth available.\n",
        "    - video_or_image_list: list of image paths OR path to single video\n",
        "    - known_heights_m: dict class_id -> real height (meters)\n",
        "    Returns: dict with avg_infer_ms, fps, distance_metrics (MAE, RMSE) if distances computed\n",
        "    Note: This function attempts to match predicted bbox to ground-truth bboxes if a GT COCO JSON is provided\n",
        "    \"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    if device:\n",
        "        model.to(device)\n",
        "\n",
        "    is_video = isinstance(video_or_image_list, str) and video_or_image_list.lower().endswith(('.mp4','.mov','.avi','.mkv'))\n",
        "    infer_times = []\n",
        "    distance_errors = []  # store absolute error meters\n",
        "    squared_errors = []\n",
        "\n",
        "    # If we have dataset GT distances per image/annotation, use them. Otherwise compute by matching to COCO annotations (not included here).\n",
        "    # For simplicity: assume we have a mapping `gt_distances_by_frame_and_track` or we compute from GT bbox heights and known real heights.\n",
        "\n",
        "    if is_video:\n",
        "        cap = cv2.VideoCapture(video_or_image_list)\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame_idx += 1\n",
        "            t0 = time.time()\n",
        "            results = model.predict(frame, conf=conf, verbose=False)\n",
        "            t1 = time.time()\n",
        "            infer_times.append((t1-t0)*1000)  # ms\n",
        "\n",
        "            # compute distance per detection using box height\n",
        "            if results and len(results) and results[0].boxes is not None:\n",
        "                boxes = results[0].boxes.xyxy.cpu().numpy()  # [N,4]\n",
        "                cls_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "                for (x1,y1,x2,y2), cls in zip(boxes, cls_ids):\n",
        "                    h_px = float(y2-y1)\n",
        "                    obj_h_m = known_heights_m.get(int(cls), default_height_m) if known_heights_m else default_height_m\n",
        "                    est_dist = (obj_h_m * focal_length_px) / (h_px + 1e-6)\n",
        "                    # If you have GT distance: compute error. Here we skip if no GT.\n",
        "                    # Example placeholder: gt_dist = get_gt_distance_for_frame(frame_idx, cls)\n",
        "                    # if gt_dist is not None: distance_errors.append(abs(est_dist - gt_dist)); squared_errors.append((est_dist-gt_dist)**2)\n",
        "            # else continue\n",
        "        cap.release()\n",
        "    else:\n",
        "        for img_path in video_or_image_list:\n",
        "            img = cv2.imread(img_path)\n",
        "            t0 = time.time()\n",
        "            results = model.predict(img, conf=conf, verbose=False)\n",
        "            t1 = time.time()\n",
        "            infer_times.append((t1-t0)*1000)\n",
        "            # same distance estimate block as above...\n",
        "    # summary:\n",
        "    avg_ms = float(np.mean(infer_times)) if infer_times else None\n",
        "    fps = 1000.0/avg_ms if avg_ms and avg_ms>0 else None\n",
        "    out = {'avg_infer_ms': avg_ms, 'fps': fps}\n",
        "    if distance_errors:\n",
        "        out['distance_MAE'] = float(np.mean(distance_errors))\n",
        "        out['distance_RMSE'] = float(np.sqrt(np.mean(squared_errors)))\n",
        "    return out\n",
        "\n",
        "# ---------- 3) Prepare a compact compare() to run both models and collect major metrics ----------\n",
        "def compare_models(baseline_model_path, tuned_model_path, data_yaml, imgsz=640, conf=0.25, iou=0.5, device=None):\n",
        "    results = {}\n",
        "    for name, mp in [('baseline', baseline_model_path), ('tuned', tuned_model_path)]:\n",
        "        res, met = run_yolov8_val(mp, data_yaml, imgsz=imgsz, conf=conf, iou=iou, device=device)\n",
        "        results[name] = {'metrics': met, 'val_result_obj': res}\n",
        "    # Build a summary dataframe\n",
        "    rows = []\n",
        "    keys = set()\n",
        "    for k in results:\n",
        "        metrics = results[k]['metrics']\n",
        "        for kk in metrics: keys.add(kk)\n",
        "    for k in results:\n",
        "        row = {'model': k}\n",
        "        for kk in keys:\n",
        "            row[kk] = results[k]['metrics'].get(kk, None)\n",
        "        rows.append(row)\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df, results\n",
        "\n",
        "# ---------- 4) Example usage ----------\n",
        "# BASELINE = '/content/drive/.../yolov8n_baseline.pt'\n",
        "# TUNED = '/content/drive/.../yolov8n_custom_coco_best.pt'\n",
        "# DATA_YAML = '/content/drive/.../data.yaml'  # must reference validation set and classes\n",
        "# df_summary, raw = compare_models(BASELINE, TUNED, DATA_YAML, imgsz=640, conf=0.25, iou=0.5, device='cuda')\n",
        "# print(df_summary)\n",
        "# df_summary.to_csv(\"model_comparison_summary.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import motmetrics as mm\n",
        "\n",
        "def compute_tracking_metrics(gt_file, hyp_file):\n",
        "    # Both CSVs: columns: [frame, id, x, y, w, h] (MOT format requires tlwh)\n",
        "    gt = mm.io.loadtxt(gt_file, fmt='mot15-2D')\n",
        "    hyp = mm.io.loadtxt(hyp_file, fmt='mot15-2D')\n",
        "    acc = mm.utils.compare_to_groundtruth(gt, hyp, 'iou', distth=0.5)\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=['mota','idf1','precision','recall','num_frames','num_objects','mostly_tracked','id_switches'], name='summary')\n",
        "    print(summary)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "mlUIJIu-8hVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confs = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "rows = []\n",
        "for c in confs:\n",
        "    df_summary, _ = compare_models(BASELINE, TUNED, DATA_YAML, imgsz=640, conf=c, iou=0.5, device='cuda')\n",
        "    # df_summary has precision, recall, mAP columns\n",
        "    df_summary['conf'] = c\n",
        "    rows.append(df_summary)\n",
        "big = pd.concat(rows, ignore_index=True)\n",
        "big.to_csv(\"threshold_sweep_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "nKDtSc668ibK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full working pipeline Need to be fixed\n"
      ],
      "metadata": {
        "id": "S8G0mLy_-Bym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline integrating YOLOv8 (ultralytics) + ByteTrack + gTTS audio alerts\n",
        "import os, time, base64, subprocess, math\n",
        "from collections import deque, defaultdict\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ByteTrack import (from installed ByteTrack package)\n",
        "try:\n",
        "    # ByteTrack repo provides BYTETracker under yolox/ or bytetrack package depending on install\n",
        "    from yolox.tracker.byte_tracker import BYTETracker\n",
        "except Exception as e:\n",
        "    try:\n",
        "        from bytetrack.byte_tracker import BYTETracker\n",
        "    except Exception as e2:\n",
        "        raise ImportError(\n",
        "            \"BYTETracker not found. Make sure you installed ByteTrack (see install cell). \"\n",
        "            \"After installing, restart the runtime and re-run this cell.\"\n",
        "        )\n",
        "\n",
        "# ---------------------- User-configurable settings ----------------------\n",
        "CUSTOM_YOLO_MODEL_PATH = '/content/drive/My Drive/VisionAssist-Models/yolov8n_custom_coco_best.pt'\n",
        "VIDEO_SOURCE = 'test.mp4'   # uploaded/placed in working dir\n",
        "SILENT_VIDEO_OUTPUT = 'temp_silent_video_mp4v.mp4'\n",
        "FINAL_VIDEO_OUTPUT = 'final_video_with_audio.mp4'\n",
        "FINAL_AUDIO_FILE = 'final_audio.mp3'\n",
        "TEMP_TTS_FILE = 'temp_alert.mp3'\n",
        "\n",
        "# Distance estimation / classes\n",
        "ESTIMATED_FOCAL_LENGTH_PIXELS = 1000\n",
        "KNOWN_OBJECT_HEIGHTS_METERS = {0:1.7, 1:1.0, 2:1.5, 3:1.2, 5:3.0, 7:3.5}\n",
        "DEFAULT_KNOWN_HEIGHT = 1.5\n",
        "\n",
        "# Alert & tracking params\n",
        "CONF_THRESHOLD = 0.35\n",
        "IOU_THRESHOLD_FOR_TRACK = 0.3    # passed to BYTETracker config\n",
        "ALERT_CLASS_COOLDOWN_SEC = 8.0\n",
        "ALERT_REPEAT_DELAY_SEC = 15.0\n",
        "HISTORY_FRAMES = 15\n",
        "MOVEMENT_THRESHOLD_PIXELS = 5\n",
        "DIRECTION_THRESHOLD_RATIO = 0.3\n",
        "\n",
        "# ByteTrack tracker parameters (tune as needed)\n",
        "BYTE_TRACKER_ARGS = {\n",
        "    'track_thresh': 0.5,   # detection confidence threshold used inside ByteTrack\n",
        "    'match_thresh': 0.8,\n",
        "    'track_buffer': 30,\n",
        "    'frame_rate': 30,\n",
        "}\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# small helpers\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def estimate_distance(box_height_px, object_real_height_m, focal_length_px):\n",
        "    if box_height_px <= 0: return float('inf')\n",
        "    return (object_real_height_m * focal_length_px) / (box_height_px + 1e-6)\n",
        "\n",
        "def get_direction_motion(track_id, frame_width, track_histories):\n",
        "    direction_str = \"Ahead\"\n",
        "    motion_str = \"Static\"\n",
        "    if track_id in track_histories and len(track_histories[track_id]) == HISTORY_FRAMES:\n",
        "        history = track_histories[track_id]\n",
        "        oldest_pos, newest_pos = history[0], history[-1]\n",
        "        dx = newest_pos[0] - oldest_pos[0]\n",
        "        dy = newest_pos[1] - oldest_pos[1]\n",
        "        if abs(dx) > MOVEMENT_THRESHOLD_PIXELS or abs(dy) > MOVEMENT_THRESHOLD_PIXELS:\n",
        "            motion_str = \"Moving\"\n",
        "        frame_center_x = frame_width / 2\n",
        "        relative_pos = (newest_pos[0] - frame_center_x) / frame_center_x\n",
        "        if relative_pos > DIRECTION_THRESHOLD_RATIO:\n",
        "            direction_str = \"Right\"\n",
        "        elif relative_pos < -DIRECTION_THRESHOLD_RATIO:\n",
        "            direction_str = \"Left\"\n",
        "    return direction_str, motion_str\n",
        "\n",
        "# Load models\n",
        "print(\"Device:\", get_device())\n",
        "DEVICE = get_device()\n",
        "\n",
        "print(\"Loading YOLO model:\", CUSTOM_YOLO_MODEL_PATH)\n",
        "yolo_model = YOLO(CUSTOM_YOLO_MODEL_PATH)\n",
        "yolo_model.to(DEVICE)\n",
        "\n",
        "# Create ByteTracker\n",
        "# ByteTracker's constructor signature (common): BYTETracker(opt) or BYTETracker(args) depending on install.\n",
        "# We'll attempt common patterns:\n",
        "def make_bytetrack(frame_rate=30):\n",
        "    try:\n",
        "        tracker = BYTETracker(BYTE_TRACKER_ARGS)\n",
        "        return tracker\n",
        "    except Exception:\n",
        "        try:\n",
        "            tracker = BYTETracker(frame_rate=frame_rate)\n",
        "            return tracker\n",
        "        except Exception as e:\n",
        "            # Fallback: try passing unpacked dict\n",
        "            try:\n",
        "                tracker = BYTETracker(**BYTE_TRACKER_ARGS)\n",
        "                return tracker\n",
        "            except Exception as e2:\n",
        "                raise RuntimeError(\"Failed to instantiate BYTETracker. See ByteTrack install instructions.\") from e2\n",
        "\n",
        "tracker = make_bytetrack(frame_rate=BYTE_TRACKER_ARGS.get('frame_rate',30))\n",
        "\n",
        "# Video I/O\n",
        "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
        "if not cap.isOpened():\n",
        "    raise FileNotFoundError(f\"Cannot open video {VIDEO_SOURCE}\")\n",
        "\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or BYTE_TRACKER_ARGS.get('frame_rate', 30)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "total_duration_ms = ((total_frames / fps) * 1000) if fps>0 else 0\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "writer = cv2.VideoWriter(SILENT_VIDEO_OUTPUT, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# runtime state\n",
        "track_histories = {}        # track_id -> deque of (cx,cy)\n",
        "alert_log = []              # list of (timestamp_ms, alert_text)\n",
        "alerted_tracks = {}         # track_id -> last_alert_time_sec\n",
        "last_alert_time_by_class = {} # class_id -> last alert time (sec)\n",
        "alert_sound_cache = {}      # alert_text -> AudioSegment\n",
        "\n",
        "frame_idx = 0\n",
        "print(\"Starting processing...\")\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "    current_msec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "    current_sec = current_msec / 1000.0\n",
        "\n",
        "    # 1) Run detection with YOLOv8\n",
        "    # We request results as numpy arrays; ultralytics may return a list of Result objects\n",
        "    results = yolo_model.predict(frame, conf=CONF_THRESHOLD, imgsz=640, verbose=False)  # single image predict\n",
        "    detections_for_tracker = []  # format: [ [x1,y1,x2,y2,score,cls], ... ] or [ [x1,y1,x2,y2,score] ] depending on ByteTrack API\n",
        "    detections_per_frame = []    # keep class info\n",
        "\n",
        "    if results and len(results) and hasattr(results[0], 'boxes') and results[0].boxes is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu().numpy()  # Nx4\n",
        "        scores = results[0].boxes.conf.cpu().numpy() # Nx\n",
        "        cls_ids = results[0].boxes.cls.cpu().numpy().astype(int)  # Nx\n",
        "        for (x1,y1,x2,y2), sc, cls in zip(boxes, scores, cls_ids):\n",
        "            # ByteTrack expects tlbr format (x1,y1,x2,y2) and score\n",
        "            detections_for_tracker.append([float(x1), float(y1), float(x2), float(y2), float(sc)])\n",
        "            detections_per_frame.append({'box':(int(x1),int(y1),int(x2),int(y2)), 'score':float(sc), 'class_id':int(cls)})\n",
        "\n",
        "    # 2) Feed ByteTrack with detections (ByteTrack typically expects numpy array)\n",
        "    import numpy as _np\n",
        "    if len(detections_for_tracker) > 0:\n",
        "        dets_np = _np.asarray(detections_for_tracker, dtype=_np.float32)\n",
        "    else:\n",
        "        dets_np = _np.zeros((0,5), dtype=_np.float32)\n",
        "\n",
        "    # ByteTrack update: signature often tracker.update(dets, img_info, frame_id=frame_idx)\n",
        "    try:\n",
        "        online_targets = tracker.update(dets_np, [frame_height, frame_width], frame_idx)\n",
        "    except TypeError:\n",
        "        # alternate signature: tracker.update(dets, frame_id=frame_idx)\n",
        "        try:\n",
        "            online_targets = tracker.update(dets_np, frame_idx)\n",
        "        except Exception as e:\n",
        "            # alternate signature: tracker.update(dets)\n",
        "            online_targets = tracker.update(dets_np)\n",
        "\n",
        "    # online_targets is usually a list of Track objects with attributes: tlwh, tlbr, track_id, score, cls\n",
        "    active_track_ids_this_frame = set()\n",
        "    # map tracker boxes to classes & scores by IoU because ByteTrack strips class info in many implementations\n",
        "    # We'll perform a simple matching: for each online_target find best matching detection_per_frame by IoU\n",
        "    def iou(boxA, boxB):\n",
        "        xA = max(boxA[0], boxB[0])\n",
        "        yA = max(boxA[1], boxB[1])\n",
        "        xB = min(boxA[2], boxB[2])\n",
        "        yB = min(boxA[3], boxB[3])\n",
        "        interW = max(0, xB - xA)\n",
        "        interH = max(0, yB - yA)\n",
        "        interArea = interW * interH\n",
        "        boxAArea = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
        "        boxBArea = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
        "        union = boxAArea + boxBArea - interArea + 1e-6\n",
        "        return interArea / union\n",
        "\n",
        "    mapped_targets = []\n",
        "    if hasattr(online_targets, '__iter__'):\n",
        "        for t in online_targets:\n",
        "            # t may be a custom object or tuple; attempt to extract bounding box and id\n",
        "            try:\n",
        "                tlbr = getattr(t, 'tlbr', None)\n",
        "                if tlbr is None:\n",
        "                    tlbr = getattr(t, 'tlwh', None)\n",
        "                    if tlbr is not None and len(tlbr) == 4:\n",
        "                        x,y,w,h = tlbr\n",
        "                        tlbr = [x, y, x+w, y+h]\n",
        "                track_id = int(getattr(t, 'track_id', getattr(t, 'id', -1)))\n",
        "                score = float(getattr(t, 'score', 0.0))\n",
        "            except Exception:\n",
        "                # fallback if t is tuple-like\n",
        "                try:\n",
        "                    arr = np.asarray(t)\n",
        "                    tlbr = arr[:4].tolist()\n",
        "                    track_id = int(arr[4])\n",
        "                    score = float(arr[5]) if arr.shape[0] > 5 else 0.0\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if tlbr is None: continue\n",
        "            mapped_targets.append({'tlbr': [float(v) for v in tlbr], 'track_id':track_id, 'score':score})\n",
        "\n",
        "    # Match mapped_targets to detections to obtain class_id\n",
        "    for mt in mapped_targets:\n",
        "        best_iou = 0.0\n",
        "        best_cls = None\n",
        "        best_box = None\n",
        "        for det in detections_per_frame:\n",
        "            i = iou(mt['tlbr'], det['box'])\n",
        "            if i > best_iou:\n",
        "                best_iou = i\n",
        "                best_cls = det['class_id']\n",
        "                best_box = det['box']\n",
        "        # If poor IoU match, still use bounding box center from tracker box\n",
        "        tl = mt['tlbr']\n",
        "        x1,y1,x2,y2 = map(int,tl)\n",
        "        cx = (x1 + x2)//2\n",
        "        cy = (y1 + y2)//2\n",
        "        tid = mt['track_id']\n",
        "        active_track_ids_this_frame.add(tid)\n",
        "        if tid not in track_histories:\n",
        "            track_histories[tid] = deque(maxlen=HISTORY_FRAMES)\n",
        "        track_histories[tid].append((cx, cy))\n",
        "\n",
        "        # estimate distance & do alerting\n",
        "        box_h_px = y2 - y1\n",
        "        cls = best_cls if best_cls is not None else -1\n",
        "        obj_real_h = KNOWN_OBJECT_HEIGHTS_METERS.get(int(cls), DEFAULT_KNOWN_HEIGHT)\n",
        "        est_dist = estimate_distance(box_h_px, obj_real_h, ESTIMATED_FOCAL_LENGTH_PIXELS)\n",
        "        direction_str, motion_str = get_direction_motion(tid, frame_width, track_histories)\n",
        "        class_name = yolo_model.names.get(int(cls), f\"Class{cls}\") if cls != -1 else \"Unknown\"\n",
        "        info_text = f\"ID:{tid} {class_name} Est:{est_dist:.1f}m {direction_str}\"\n",
        "        if motion_str == \"Moving\":\n",
        "            info_text += \" Moving\"\n",
        "        # Draw box & label\n",
        "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(frame, info_text, (x1, max(20,y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "\n",
        "        # Alert logic: person <2m, objects <5m (same as original)\n",
        "        is_person = (cls == 0)\n",
        "        is_close_person = is_person and (est_dist < 2.0)\n",
        "        is_close_object = (not is_person) and (est_dist < 5.0)\n",
        "        if (is_close_person or is_close_object) and frame_idx > 10:\n",
        "            last_alerted_time_for_track = alerted_tracks.get(tid, -1e9)\n",
        "            if current_sec - last_alerted_time_for_track > ALERT_REPEAT_DELAY_SEC:\n",
        "                # per-class cooldown\n",
        "                last_time_for_class = last_alert_time_by_class.get(int(cls), -1e9)\n",
        "                if current_sec - last_time_for_class > ALERT_CLASS_COOLDOWN_SEC:\n",
        "                    dist_str = f\"{est_dist:.0f}\"\n",
        "                    alert_base = f\"{class_name}, about {dist_str} meters, {direction_str}.\"\n",
        "                    alert_text_this_frame = f\"Caution: {alert_base}\"\n",
        "                    alert_log.append((int(current_msec), alert_text_this_frame))\n",
        "                    alerted_tracks[tid] = current_sec\n",
        "                    last_alert_time_by_class[int(cls)] = current_sec\n",
        "                    print(f\"[ALERT @ {current_sec:.2f}s] {alert_text_this_frame}\")\n",
        "\n",
        "    # cleanup track_histories for disappeared tracks\n",
        "    current_track_ids_set = set(active_track_ids_this_frame)\n",
        "    for tid in list(track_histories.keys()):\n",
        "        if tid not in current_track_ids_set:\n",
        "            del track_histories[tid]\n",
        "            if tid in alerted_tracks:\n",
        "                del alerted_tracks[tid]\n",
        "\n",
        "    # write frame to silent video\n",
        "    writer.write(frame)\n",
        "\n",
        "# finished loop\n",
        "cap.release()\n",
        "writer.release()\n",
        "print(\"Video processing finished. Silent video saved:\", SILENT_VIDEO_OUTPUT)\n",
        "\n",
        "# ------------------- Generate audio from alerts using gTTS -------------------\n",
        "print(\"Generating TTS audio for unique alerts...\")\n",
        "final_audio = AudioSegment.silent(duration=int(total_duration_ms) if total_duration_ms>0 else 1000)\n",
        "\n",
        "unique_alerts = list({text for _, text in alert_log})\n",
        "for text in unique_alerts:\n",
        "    try:\n",
        "        tts = gTTS(text=text, lang='en', slow=False)\n",
        "        tts.save(TEMP_TTS_FILE)\n",
        "        alert_sound_cache[text] = AudioSegment.from_mp3(TEMP_TTS_FILE)\n",
        "        os.remove(TEMP_TTS_FILE)\n",
        "    except Exception as e:\n",
        "        print(\"gTTS warning:\", e)\n",
        "\n",
        "print(\"Overlaying alerts onto timeline...\")\n",
        "for timestamp_ms, text in alert_log:\n",
        "    if text in alert_sound_cache:\n",
        "        sound = alert_sound_cache[text]\n",
        "        final_audio = final_audio.overlay(sound, position=int(timestamp_ms))\n",
        "\n",
        "final_audio.export(FINAL_AUDIO_FILE, format='mp3')\n",
        "print(\"Final audio saved:\", FINAL_AUDIO_FILE)\n",
        "\n",
        "# ------------------- Mux audio + video with ffmpeg -------------------\n",
        "print(\"Muxing audio and video using ffmpeg...\")\n",
        "ffmpeg_cmd = [\n",
        "    'ffmpeg', '-y', '-loglevel', 'error',\n",
        "    '-i', SILENT_VIDEO_OUTPUT, '-i', FINAL_AUDIO_FILE,\n",
        "    '-c:v', 'libx264', '-c:a', 'aac', '-shortest', FINAL_VIDEO_OUTPUT\n",
        "]\n",
        "try:\n",
        "    subprocess.run(ffmpeg_cmd, check=True)\n",
        "    print(\"Final video with audio saved:\", FINAL_VIDEO_OUTPUT)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"FFmpeg failed:\", e)\n",
        "\n",
        "# Optionally display inline (if running in Colab/Notebook)\n",
        "try:\n",
        "    from IPython.display import HTML, display\n",
        "    data = open(FINAL_VIDEO_OUTPUT,\"rb\").read()\n",
        "    video_url = \"data:video/mp4;base64,\" + base64.b64encode(data).decode()\n",
        "    display(HTML(f'<video width=\"640\" height=\"480\" controls><source src=\"{video_url}\" type=\"video/mp4\"></video>'))\n",
        "except Exception as e:\n",
        "    print(\"Could not inline-display video:\", e)\n",
        "\n",
        "# Cleanup (optional)\n",
        "# os.remove(SILENT_VIDEO_OUTPUT); os.remove(FINAL_AUDIO_FILE)\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "W5hxktKL9_vC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}