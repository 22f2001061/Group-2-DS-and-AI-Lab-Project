# VisionAssist - Real-Time Navigation Support for the Visually Impaired

## ğŸ§­ Project Overview

   VisionAssist is a real-time navigation aid designed to help visually impaired individuals navigate their surroundings safely. Using object detection, tracking, and distance estimation, the system provides auditory feedback about obstacles and objects in the userâ€™s path.
   The project uses a combination of **COCO 2017 dataset** and a **custom dataset** collected from **YouTube video frames** ([available here](https://drive.google.com/drive/folders/1ztLWfdN3As3kEFBYy0h9rb9OPw6CVTBp?usp=drive_link)) to train and fine-tune the YOLO model for real-world scenarios.
   The goal is to create an affordable and efficient assistive tool for enhanced independence and spatial awareness.

---

## ğŸ§© Problem Statement / Motivation

Visually impaired individuals often face challenges in perceiving their surroundings, especially in dynamic environments like streets or crowded areas. While existing assistive devices provide partial solutions, they are often expensive or lack real-time feedback. VisionAssist bridges this gap using computer vision and audio guidance to offer timely navigation support.

---

## ğŸ§± Repository Structure

```
Group-2-DS-and-AI-Lab-Project/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ data.yaml
â”‚   â”œâ”€â”€ instances_test.json
â”‚   â”œâ”€â”€ instances_train.json
â”‚   â””â”€â”€ instances_val.json
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Milestone_1.pdf
â”‚   â”œâ”€â”€ Milestone_2.pdf
â”‚   â”œâ”€â”€ Milestone_3.pdf
â”‚   â””â”€â”€ Milestone_4.pdf
â”œâ”€â”€ results/
â”‚   â””â”€â”€ eda/
â”‚       â”œâ”€â”€ aspect_ratios.png
â”‚       â”œâ”€â”€ bbox_areas.png
â”‚       â”œâ”€â”€ class_distribution.png
â”‚       â”œâ”€â”€ object_aspect_ratios.png
â”‚       â”œâ”€â”€ object_locations_heatmap.png
â”‚       â””â”€â”€ objects_per_image.png
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_loading/
â”‚   â”‚   â”œâ”€â”€ .gitignore
â”‚   â”‚   â”œâ”€â”€ Custom Data Collection Script.ipynb
â”‚   â”‚   â””â”€â”€ dataset_sample_collection_annotation.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ Main.ipynb
â”‚   â”œâ”€â”€ DSAI_eval.ipynb
â”‚   â”œâ”€â”€ EDA_MS_COCO.ipynb
â”‚   â””â”€â”€ Hyperparametertuning.ipynb
â”œâ”€â”€ DATA_GOVERNANCE.md
â””â”€â”€ README.md
```
---

## ğŸš€ Usage Guide

This section provides instructions on how to run and reproduce key parts of the VisionAssist project.

---

### ğŸ§  Model Training

Follow these steps to train the VisionAssist detection model using the [Main.ipynb](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/scripts/training/Main.ipynb) notebook.
These steps outline the workflow for dataset preparation, fine-tuning YOLOv8, and saving trained weights.

1. **Mount Google Drive in Colab**

   * Mount your Google Drive to access datasets.
   * Confirm that the shared **YouTube frames dataset** path exists ([Google Drive link](https://drive.google.com/drive/folders/1ztLWfdN3As3kEFBYy0h9rb9OPw6CVTBp?usp=drive_link)).
   * If not, adjust the directory path accordingly.

2. **Unzip COCO Data**

   * Extract `filtered_coco_data.zip` from Drive into a temporary folder.
   * Locate the extracted `images` and (if available) `labels` directories.

3. **Combine Datasets**

   * Create a folder named `master_dataset/images`.
   * Copy all COCO images into it.
   * Merge the **YouTube frame images** from the custom dataset.
   * Verify the total number of combined images.

4. **Auto-Annotate Using YOLOv8**

   * Install **Ultralytics** using:

     ```bash
     pip install ultralytics
     ```
   * Load the pretrained `yolov8n.pt` model.
   * Auto-annotate all images in `master_dataset/images` to generate YOLO TXT labels in `master_dataset/labels`.

5. **Split the Dataset**

   * Create `split_dataset/train`, `split_dataset/valid`, and `split_dataset/test` subfolders for both `images` and `labels`.
   * Shuffle and distribute files in a **70/20/10** ratio, ensuring each image has its corresponding label file.

6. **Generate Dataset Config YAML**

   * Create a `coco_custom_data.yaml` file that points to the split directories.
   * Include all 80 COCO classes in the `names:` section.

7. **Train YOLOv8 Model**

   * Run YOLOv8 training with desired hyperparameters:

     ```bash
     yolo task=detect mode=train model=yolov8n.pt data=coco_custom_data.yaml epochs=50 imgsz=640
     ```

8. **Save Trained Weights**

   * After training, copy the resulting `best.pt` file from `runs/detect/...` to your Google Drive for safekeeping.



---
ğŸ“„ **Milestone Documents**

All official milestone submissions are located in the [`docs/`](./docs) directory of the repository:

* **[Milestone_1.pdf](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/docs/Milestone_1.pdf)** â€“ Covers dataset selection, problem statement, and preliminary findings.
* **[Milestone_2.pdf](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/docs/Milestone_2.pdf)** â€“ Details dataset preparation, preprocessing, exploration, and custom data collection.
* **[Milestone_3.pdf](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/docs/Milestone_3.pdf)** â€“ Focuses on model selection, architecture choice, and training methodology.
* **[Milestone_4.pdf](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/docs/Milestone_4.pdf)** â€“ Documents model training, hyperparameter tuning, evaluation, and application-level experimentation.
* **[Milestone_4 v2.pdf](https://github.com/22f2001061/Group-2-DS-and-AI-Lab-Project/blob/main/docs/Milestone_4 v2.pdf)** â€“ Updates as per the feedback recieved on the M4 submission done in earlier iteration.


---
## ğŸ§° Technology Stack / Tools Used

   * **YOLOv8 (Ultralytics)** â€“ Object detection
   * **ByteTrack** â€“ Multi-object tracking
   * **gTTS (Google Text-to-Speech)** â€“ Voice feedback
   * **OpenCV** â€“ Image and video processing
   * **Python** â€“ Core programming language
   * **MS COCO Dataset** â€“ Base dataset for model training and benchmarking
   * **[Custom YouTube Frame Dataset](https://drive.google.com/drive/folders/1ztLWfdN3As3kEFBYy0h9rb9OPw6CVTBp?usp=drive_link)** â€“ Additional dataset curated for fine-tuning and real-world diversity
   * **Jupyter Notebooks** â€“ Development and experimentation environment

---


## ğŸ§  Model Description / Methodology

1. **Object Detection:** YOLOv8 detects objects in each video frame.
2. **Tracking:** ByteTrack assigns consistent IDs for moving objects.
3. **Distance Estimation:** Uses bounding box size and focal length for approximate object distance.
4. **Audio Feedback:** gTTS converts detections into spoken alerts for the user.

---

## ğŸ§© System Architecture / Workflow

1. Video Frame â†’ YOLOv8 Detection â†’ ByteTrack Tracking
2. Tracking Data â†’ Distance Estimation â†’ gTTS Audio Output
3. User receives real-time spoken navigation cues

---

## ğŸ‘¥ Team Members

* Balasurya K
* Jivraj Singh Shekhawat
* Tanuja Nair
* Karan Patil
* Prashasti Sarraf

---

## âš–ï¸ License

### a) Project License

This project is released under the **MIT License** â€” free for research and educational use.

### b) Dataset Credits and License

Annotations and dataset formats reference the **MS COCO dataset**, licensed under the **Creative Commons Attribution 4.0 License (CC BY 4.0)**.

---

## ğŸ™ Acknowledgements

We thank our instructors for their continuous guidance and the open-source community for providing powerful tools like Ultralytics YOLO and ByteTrack that made this project possible.
