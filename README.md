# VisionAssist - Real-Time Navigation Support for the Visually Impaired

## ğŸ§­ Project Overview

VisionAssist is a real-time navigation aid designed to help visually impaired individuals navigate their surroundings safely. Using object detection, tracking, and distance estimation, the system provides auditory feedback about obstacles and objects in the userâ€™s path. The goal is to create an affordable and efficient assistive tool for enhanced independence and spatial awareness.

---

## ğŸ§© Problem Statement / Motivation

Visually impaired individuals often face challenges in perceiving their surroundings, especially in dynamic environments like streets or crowded areas. While existing assistive devices provide partial solutions, they are often expensive or lack real-time feedback. VisionAssist bridges this gap using computer vision and audio guidance to offer timely navigation support.

---

## ğŸ§± Repository Structure

```
Group-2-DS-and-AI-Lab-Project/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ data.yaml
â”‚   â”œâ”€â”€ instances_test.json
â”‚   â”œâ”€â”€ instances_train.json
â”‚   â””â”€â”€ instances_val.json
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Milestone_1.pdf
â”‚   â”œâ”€â”€ Milestone_2.pdf
â”‚   â”œâ”€â”€ Milestone_3.pdf
â”‚   â””â”€â”€ Milestone_4.pdf
â”œâ”€â”€ results/
â”‚   â””â”€â”€ eda/
â”‚       â”œâ”€â”€ aspect_ratios.png
â”‚       â”œâ”€â”€ bbox_areas.png
â”‚       â”œâ”€â”€ class_distribution.png
â”‚       â”œâ”€â”€ object_aspect_ratios.png
â”‚       â”œâ”€â”€ object_locations_heatmap.png
â”‚       â””â”€â”€ objects_per_image.png
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_loading/
â”‚   â”‚   â”œâ”€â”€ .gitignore
â”‚   â”‚   â”œâ”€â”€ Custom Data Collection Script.ipynb
â”‚   â”‚   â””â”€â”€ dataset_sample_collection_annotation.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ Main.ipynb
â”‚   â”œâ”€â”€ DSAI_eval.ipynb
â”‚   â”œâ”€â”€ EDA_MS_COCO.ipynb
â”‚   â””â”€â”€ Hyperparametertuning.ipynb
â”œâ”€â”€ DATA_GOVERNANCE.md
â””â”€â”€ README.md
```

---

## ğŸ§° Technology Stack / Tools Used

* **YOLOv8 (Ultralytics)** â€“ Object detection
* **ByteTrack** â€“ Multi-object tracking
* **gTTS (Google Text-to-Speech)** â€“ Voice feedback
* **OpenCV** â€“ Image and video processing
* **Python** â€“ Core programming language
* **MS COCO Dataset** â€“ Pre-trained annotation structure reference
* **Jupyter Notebooks** â€“ Development and experimentation environment

---


## ğŸ§  Model Description / Methodology

1. **Object Detection:** YOLOv8 detects objects in each video frame.
2. **Tracking:** ByteTrack assigns consistent IDs for moving objects.
3. **Distance Estimation:** Uses bounding box size and focal length for approximate object distance.
4. **Audio Feedback:** gTTS converts detections into spoken alerts for the user.

---

## ğŸ§© System Architecture / Workflow

1. Video Frame â†’ YOLOv8 Detection â†’ ByteTrack Tracking
2. Tracking Data â†’ Distance Estimation â†’ gTTS Audio Output
3. User receives real-time spoken navigation cues

---

## ğŸ‘¥ Team Members

* Balasurya K
* Jivraj Singh Shekhawat
* Tanuja Nair
* Karan Patil
* Prashasti Sarraf

---

## âš–ï¸ License

### a) Project License

This project is released under the **MIT License** â€” free for research and educational use.

### b) Dataset Credits and License

Annotations and dataset formats reference the **MS COCO dataset**, licensed under the **Creative Commons Attribution 4.0 License (CC BY 4.0)**.

---

## ğŸ™ Acknowledgements

We thank our instructors for their continuous guidance and the open-source community for providing powerful tools like Ultralytics YOLO and ByteTrack that made this project possible.
